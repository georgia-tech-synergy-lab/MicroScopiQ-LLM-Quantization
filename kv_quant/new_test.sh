python evaluation_gsm8k.py --model meta-llama/Meta-Llama-3-8B --prompt_file gsm8k_prompt_original.txt --batch_size 8 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64 > llama3_gsm8k_gearlkivi4b.txt
python evaluation_gsm8k.py --model mistralai/Mistral-7B-v0.1 --prompt_file gsm8k_prompt_original.txt --batch_size 8 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64 > mistral_gsm8k_gearlkivi4b.txt
python evaluation_gsm8k.py --model meta-llama/Llama-2-13b-hf --prompt_file gsm8k_prompt_original.txt --batch_size 8 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64 > llama2_gsm8k_gearlkivi4b.txt

python evaluation_aqua_cot.py --model meta-llama/Meta-Llama-3-8B --batch_size 6 --max_new_tokens 196 --model_max_length 4096 --root_output_dir ./aqua --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --group_size 64 --stream_grouping --streaming --streaming_gap 64 > llama3_newgearlkivi4b_aqua.txt
python evaluation_aqua_cot.py --model mistralai/Mistral-7B-v0.1 --batch_size 6 --max_new_tokens 196 --model_max_length 4096 --root_output_dir ./aqua --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --group_size 64 --stream_grouping --streaming --streaming_gap 64 > mistral_newgearlkivi4b_aqua.txt
python evaluation_aqua_cot.py --model meta-llama/Llama-2-13b-hf --batch_size 6 --max_new_tokens 196 --model_max_length 4096 --root_output_dir ./aqua --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --group_size 64 --stream_grouping --streaming --streaming_gap 64 > llama2_newgearlkivi4b_aqua.txt

python evaluation_bbh_cot.py --model meta-llama/Meta-Llama-3-8B --batch_size 4 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64  > llama3_bbh_newgearlkivi_4b.txt
python evaluation_bbh_cot.py --model mistralai/Mistral-7B-v0.1 --batch_size 4 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64  > mistral_bbh_newgearlkivi_4b.txt
python evaluation_bbh_cot.py --model meta-llama/Llama-2-13b-hf --batch_size 4 --max_new_tokens 256 --compress_method NEWGEARLKIVI --attention_number 40 --quantize_bit 4 --prefillrank 4 --group_size 64 --prefillrankv 4 --rank 2 --rankv 2 --loop 3 --streaming --stream_grouping --streaming_gap 64  > llama2_bbh_newgearlkivi_4b.txt



